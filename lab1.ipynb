{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f208102c-5a85-4ddc-b91b-68b9458b5381",
   "metadata": {},
   "source": [
    "Zachary Proom\n",
    "\n",
    "EN.605.646.81: Natural Language Processing\n",
    "\n",
    "# Lab #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb8e40ef-2b13-41fc-97ec-2eb7fb398425",
   "metadata": {},
   "outputs": [],
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3c45c8a-fef1-483f-99b4-cb726782b4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define the tokenizer function.\n",
    "def tokenize_text(input):\n",
    "    \"\"\"        \n",
    "    Parameters\n",
    "    ----------\n",
    "    input : str\n",
    "        The input text that gets tokenized.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    An ordered list of normalized tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add whitespace around punctuation. Exclude periods and hyphens for now\n",
    "    # because they require extra care.\n",
    "    input_nopunct = re.sub(\"(\\.\\.\\.)\", r\" \\1 \", input) # Ellipsis\n",
    "    input_nopunct = re.sub(\"([^\\w\\d\\.-])\", r\" \\1 \", input_nopunct)\n",
    "\n",
    "    # Add whitespace around a period only when it's at the end of a sentence.\n",
    "    # To detect this, check if the following conditions are true:\n",
    "    #     (1) There is whitespace after the period.\n",
    "    #     (2) The period is at the end of a line.\n",
    "    #     (3) There is a lowercase letter before the period, and an uppercase\n",
    "    #         letter after the period.\n",
    "    # In cases (1) and (2), the period cannot be preceded by a period because\n",
    "    # it would be part of an ellipsis.\n",
    "    input_nopunct = re.sub(\"(?<![\\.])(\\.)\\s\", r\" \\1 \", input_nopunct)\n",
    "    input_nopunct = re.sub(\"(?<![\\.])(\\.)$\", r\" \\1 \", input_nopunct)\n",
    "    input_nopunct = re.sub(\"([a-z])(\\.)([A-Z])\", r\"\\1 \\2 \\3\", input_nopunct)\n",
    "\n",
    "    # Add whitespace around a hyphen except when it's used between letters \n",
    "    # (e.g. so-called).\n",
    "    input_nopunct = re.sub(r'(?<![a-zA-Z])-(?![a-zA-Z])', ' - ', input_nopunct)\n",
    "    \n",
    "    # Lower-case all the words in the input text.\n",
    "    input_lowercase = input_nopunct.lower()\n",
    "    \n",
    "    # Split all the words by whitespace.\n",
    "    input_wssplit = input_lowercase.split()\n",
    "    \n",
    "    return input_wssplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eefdf3e9-a1f4-4b64-afdc-90985ed5b3e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nac',\n",
       " 'has',\n",
       " 'developed',\n",
       " 'a',\n",
       " 'national',\n",
       " 'hiv',\n",
       " '/',\n",
       " 'aids',\n",
       " '/',\n",
       " 'sti',\n",
       " '/',\n",
       " 'tb',\n",
       " 'intervention',\n",
       " 'strategic',\n",
       " 'plan',\n",
       " '(',\n",
       " '2002',\n",
       " '-',\n",
       " '2005',\n",
       " ')',\n",
       " 'that',\n",
       " 'aims',\n",
       " 'to',\n",
       " 'reduce',\n",
       " 'the',\n",
       " 'hiv',\n",
       " 'prevalence',\n",
       " 'rate',\n",
       " 'among',\n",
       " 'zambians',\n",
       " 'from',\n",
       " '19.3',\n",
       " '%',\n",
       " 'to',\n",
       " '11.7',\n",
       " '%',\n",
       " 'and',\n",
       " 'improve',\n",
       " 'the',\n",
       " 'health',\n",
       " 'status',\n",
       " 'of',\n",
       " 'people',\n",
       " 'living',\n",
       " 'with',\n",
       " 'hiv',\n",
       " '/',\n",
       " 'aids',\n",
       " 'by',\n",
       " '2005',\n",
       " '.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the tokenizer function on the example sentence in the prompt.\n",
    "tokenize_text(\"NAC has developed a National HIV/AIDS/STI/TB Intervention Strategic Plan (2002-2005) that aims to reduce the HIV prevalence rate among Zambians from 19.3% to 11.7% and improve the health status of people living with HIV/AIDS by 2005.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fe1dec-df3c-402f-840c-5d0687db925f",
   "metadata": {},
   "source": [
    "I define a function above called tokenize_text(). It uses the re module to pad punctuation characters with spaces, so the string can be split on spaces. Before being split, the string is also converted to lowercase. The function adds whitespace around all punctuation, and it handles periods and hyphens with extra care. In particular, the function tries to only add whitespace around periods when it's at the end of a sentence. It also only tries to add whitespace around hyphens when they aren't between letters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588f6714-6c37-4846-a35c-a6bf46aca794",
   "metadata": {},
   "source": [
    "As you can see above, the function works on the test sentence. In my first attempt, I did not think about splitting around ellipses. I noticed my function didn't split ellipses on the first ten lines in tokens.txt, and I added more code to my function to handle them. I also didn't think about typos where there's no space between the last letter of a sentence and the first letter of the proceeding sentence (e.g. \"ham.I do\"). I had to add condition (3) in my function to deal with this correctly. My function still has trouble handling some less common initialisms like \"U.S. of A\". I'm unsure whether my tokenizer function should split contractions like \"they're\". Currently, it splits on the apostrophe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1158a4-82e2-48e4-ba68-dda129465ead",
   "metadata": {},
   "source": [
    "Below I show the result of processing the first ten lines of tokens.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5b684a9-67cf-4661-96e7-8adf4df8d645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['russian', 'for', 'plastic', 'bag', 'is', 'полиэтиленовый', 'пакет', '.', '7.3', 'out', 'of', '10', 'statistics', 'is', 'made', 'up', '.', 'i', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '.', 'i', 'do']\n",
      "['not', 'like', 'them', 'sam-i-am', '.', 'dr', '.', 'mulholland', 'lives', 'on', 'mulholland', 'dr', '.', 'in', 'hollywood', '.', '1', ',', '2', ',', '3', '...', 'slashdot.com', 'has', 'some', 'interesting']\n",
      "['articles', '.', 'i', \"'\", 'm', 'going', 'to', 'update', 'my', 'resumé', '.', 'j.h.u', '.', 'has', 'a', 'great', 'la-crosse', 'team', '.', 'born', 'in', 'the', 'u.s', '.', 'of', 'a', '.', 'incorrect', 'plurala', 'can', 'be']\n",
      "['fun', '.', 'is', 'capitalization', '(', 'sp', '?', ')', 'truly', 'necessary', '?', 'i', 'think', 'lower', 'case', 'is', 'more', 'legible', '.', 'when', 'people', 'write', 'in', 'all', 'caps', ',', 'it', 'feels']\n",
      "['like', 'they', \"'\", 're', 'yelling', '!', 'it', 'is', 'precisely', 'to', 'these', 'substances', 'that', 'the', 'so-called', 'french', 'paradox', 'is', 'ascribed', ':', 'although', 'the', 'french']\n",
      "['certainly', 'aren', '’', 't', 'stingy', 'with', 'food', 'and', 'don', '’', 't', 'exercise', 'much', ',', 'they', 'suffer', 'from', 'far', 'fewer', 'heart', 'attacks', 'than', 'do', 'people', 'in', 'other']\n",
      "['countries', '.', '“', 'we', '’', 've', 'made', 'some', 'real', 'serious', 'changes', 'in', 'the', 'last', 'couple', 'of', 'years', ',', 'and', 'we', '’', 've', 'made', 'a', 'lot', 'of', 'cuts', 'in', 'some', 'areas', '.']\n",
      "['the', 'feds', 'also', 'charged', 'diiorio', 'with', 'using', 'the', 'fake', 'badge', 'at', 'a', 'hotel', 'in', 'june', '.', 'this', 'is', 'the', 'first', 'song', 'i', '’', 've', 'ever', 'written', 'with']\n",
      "['lyrics', ',', 'and', 'it', '’', 's', 'an', 'anthem', 'for', 'myself', 'or', 'a', 'reminder', 'to', 'look', 'inward', '.', '\"', 'asia', 'will', 'continue', 'to', 'lead', 'global', 'ipo', 'activity', 'as', 'domestic']\n",
      "['and', 'foreign', 'ipo', 'pipeline', 'builds', '.', 'xhavit', 'haliti', ',', 'a', 'co-chairman', 'of', 'this', 'first', 'meeting', 'between', 'the', 'parliaments', ',', 'noted', 'that', 'the']\n"
     ]
    }
   ],
   "source": [
    "with open(\"tokens.txt\") as file:\n",
    "    for i in range(10):\n",
    "        line = next(file).strip()\n",
    "        print(tokenize_text(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe7a778-edc3-4aa1-adcf-7401e549fa61",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b84fb564-c985-4ba7-81ac-8fcdafa76ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_info = {\n",
    "    \"n_lines\": 0,\n",
    "    \"unique_tokens\": set(),\n",
    "    \"vocab_size\": 0,\n",
    "    \"collection_size\": 0,\n",
    "    \"unique_token_counts\": dict()\n",
    "}\n",
    "\n",
    "# Process all the lines in the text file.\n",
    "with open(\"tokens.txt\") as file:\n",
    "    for line in file:\n",
    "        summary_info[\"n_lines\"] += 1\n",
    "        result = tokenize_text(line)\n",
    "        [summary_info[\"unique_tokens\"].add(i) for i in result]\n",
    "        summary_info[\"collection_size\"] = summary_info[\"collection_size\"] + len(result)\n",
    "        for token in result:\n",
    "            if token in summary_info[\"unique_token_counts\"]:\n",
    "                summary_info[\"unique_token_counts\"][token] = summary_info[\"unique_token_counts\"].get(token, 0) + 1 \n",
    "            if token not in summary_info[\"unique_token_counts\"]:\n",
    "                summary_info[\"unique_token_counts\"][token] = 1\n",
    "    \n",
    "summary_info[\"vocab_size\"] = len(summary_info[\"unique_tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15d8f653-f622-4ccc-9e72-7f1a1d37504a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944802\n",
      "344521\n",
      "23432250\n"
     ]
    }
   ],
   "source": [
    "# Report number of lines processed.\n",
    "print(summary_info[\"n_lines\"])\n",
    "\n",
    "# Report number of unique tokens.\n",
    "print(summary_info[\"vocab_size\"])\n",
    "\n",
    "# Report collection size\n",
    "print(summary_info[\"collection_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5dcdb35a-be30-4a04-a3b5-89e93af50902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the counts.\n",
    "summary_info[\"unique_token_counts\"] = dict(sorted(summary_info[\"unique_token_counts\"].items(), key=lambda item: item[1], reverse = True))\n",
    "\n",
    "# Find the most frequent types.\n",
    "ranks_1_100 = {k: summary_info[\"unique_token_counts\"][k] for k in list(summary_info[\"unique_token_counts\"])[:100]}\n",
    "rank_500_key = list(summary_info[\"unique_token_counts\"].keys())[500]\n",
    "rank_1000_key = list(summary_info[\"unique_token_counts\"].keys())[1000]\n",
    "rank_5000_key = list(summary_info[\"unique_token_counts\"].keys())[5000]\n",
    "rank_10000_key = list(summary_info[\"unique_token_counts\"].keys())[10000]\n",
    "\n",
    "rank_500_val = summary_info[\"unique_token_counts\"][rank_500_key]\n",
    "rank_1000_val = summary_info[\"unique_token_counts\"][rank_1000_key]\n",
    "rank_5000_val = summary_info[\"unique_token_counts\"][rank_5000_key]\n",
    "rank_10000_val = summary_info[\"unique_token_counts\"][rank_10000_key]\n",
    "\n",
    "rank_500 = dict()\n",
    "rank_1000 = dict()\n",
    "rank_5000 = dict()\n",
    "rank_10000 = dict()\n",
    "\n",
    "rank_500[rank_500_key] = rank_500_val\n",
    "rank_1000[rank_1000_key] = rank_1000_val\n",
    "rank_5000[rank_5000_key] = rank_5000_val\n",
    "rank_10000[rank_10000_key] = rank_10000_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50159150-1aea-4d56-b04c-7f27fc571366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1154788, '.': 1049089, ',': 1027875, 'to': 564440, 'and': 534802, 'of': 487282, 'a': 455057, 'in': 390653, 'for': 214839, 'that': 203046, 'is': 202438, 's': 187993, '’': 187233, 'on': 164662, 'it': 156335, 'with': 149567, \"'\": 141226, 'was': 130719, 'at': 127865, '\"': 126344, 'i': 117749, 'as': 113449, 'be': 107919, '“': 106826, 'he': 104806, '”': 102222, 'are': 101822, 'said': 94457, 'this': 92847, ':': 92800, 'we': 91866, 'have': 91076, 'from': 89899, 'by': 88317, 'will': 80899, 'you': 80481, 'they': 76653, 'has': 73793, 'but': 71078, 'not': 69961, 'an': 68283, 'his': 67887, 'their': 60158, 'or': 56686, '-': 54567, 't': 52885, 'who': 52176, 'more': 49498, 'one': 48891, 'all': 48833, ')': 46853, 'she': 46822, '(': 46809, 'about': 45767, 'there': 45686, 'can': 44820, 'were': 44501, 'had': 43002, 'been': 42080, 'our': 40601, 'up': 40236, 'her': 39896, 'when': 39866, 'also': 39448, 'out': 39265, 'people': 38390, 'would': 37923, 'new': 36832, 'if': 36764, 'what': 36477, 'which': 35518, 'so': 34715, 'time': 32046, '?': 31084, 'your': 31075, 'after': 30878, 'year': 30782, 'its': 30088, 'my': 29888, 'two': 29816, 'first': 29025, 'some': 29014, 'just': 28807, 'do': 28200, 'no': 28139, 'other': 28093, 'years': 27854, 'than': 27766, 'like': 27678, '$': 27489, 'them': 27169, 'over': 26746, 'into': 26586, 'get': 23562, 'now': 22415, 'only': 22166, 'last': 22095, 'school': 22031, 'state': 21999, 'many': 21503}\n",
      "{'security': 4620}\n",
      "{'reading': 2451}\n",
      "{'deserves': 367}\n",
      "{'paired': 133}\n"
     ]
    }
   ],
   "source": [
    "# Report the most frequent types.\n",
    "print(ranks_1_100)\n",
    "print(rank_500)\n",
    "print(rank_1000)\n",
    "print(rank_5000)\n",
    "print(rank_10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84dc6fed-49ec-46d3-a5e4-958156fc0e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198077\n"
     ]
    }
   ],
   "source": [
    "# Find the number of singletons.\n",
    "n_singletons = 0\n",
    "for val in summary_info[\"unique_token_counts\"].values():\n",
    "    if val == 1:\n",
    "        n_singletons += 1\n",
    "\n",
    "# Report the number of tokens that occur once (singletons).\n",
    "print(n_singletons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b59bfbef-6c31-4e83-a98f-1977635a22de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.49344742410477\n"
     ]
    }
   ],
   "source": [
    "# Report the the percentage of the vocabulary that singletons constitute.\n",
    "print(100*n_singletons/summary_info[\"vocab_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a47d46-761d-4171-843f-a54c03ff2ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
