{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "628739d4-eda3-4434-9b6a-ffae99535986",
   "metadata": {},
   "source": [
    "Zachary Proom\n",
    "\n",
    "EN.605.646.81: Natural Language Processing\n",
    "\n",
    "# Lab #3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405a2c93-5845-4cd9-a426-c21ea608166a",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e095e2-fdb3-40ae-bbff-fb0b1336c0db",
   "metadata": {},
   "source": [
    "First, I split the training data in train.tsv into two groups, based on the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "addb988d-6528-4b94-8413-a98618c24c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc6e229b-3fc7-4b71-863f-d11cff845dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in train.tsv.\n",
    "training_data = pd.read_csv('train.tsv', sep = '\\t', header = None) \n",
    "\n",
    "# Add column names.\n",
    "training_data.columns = [\"stars\", \"docid\", \"text\"]\n",
    "\n",
    "# Split into two groups based on class.\n",
    "training_data_negative = training_data.loc[training_data[\"stars\"] == 2]\n",
    "training_data_positive = training_data.loc[training_data[\"stars\"] == 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800c3fd2-566a-4440-9d72-c8f6a674355d",
   "metadata": {},
   "source": [
    "I found ten words that indicate positive or negative sentiment, and I show their relative frequencies in the table below (i.e. the percent of reviews they appear in in each class). The first five words indicate positive sentiment, and the last five indicate negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c0e4cae-c0b8-4796-9f01-ce44695efa9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amazing</td>\n",
       "      <td>5.2</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>awesome</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>great</td>\n",
       "      <td>35.9</td>\n",
       "      <td>17.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>incredible</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fantastic</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>terrible</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>horrible</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>worst</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>awful</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bad</td>\n",
       "      <td>7.1</td>\n",
       "      <td>16.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word  positive  negative\n",
       "0     amazing       5.2       1.8\n",
       "1     awesome       5.7       2.2\n",
       "2       great      35.9      17.3\n",
       "3  incredible       0.4       0.1\n",
       "4   fantastic       3.7       1.0\n",
       "5    terrible       0.3       3.2\n",
       "6    horrible       0.6       3.1\n",
       "7       worst       0.7       4.7\n",
       "8       awful       0.6       2.5\n",
       "9         bad       7.1      16.7"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_freqs = pd.DataFrame(columns = ['word', 'positive', 'negative'])\n",
    "\n",
    "for word in [\"amazing\", \"awesome\", \"great\", \"incredible\", \"fantastic\", \"terrible\", \"horrible\", \"worst\", \"awful\", \"bad\"]:\n",
    "    negative_freq = sum(training_data_negative['text'].str.contains(word))/len(training_data_negative) * 100\n",
    "    positive_freq = sum(training_data_positive['text'].str.contains(word))/len(training_data_positive) * 100\n",
    "    relative_freqs = pd.concat([relative_freqs, pd.DataFrame({\"word\": [word], \"positive\": [positive_freq], \"negative\": [negative_freq]})], ignore_index = True)\n",
    "\n",
    "relative_freqs = relative_freqs.reset_index(drop=True)\n",
    "relative_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2fc7c7-6e19-49b2-aa50-f8c518803823",
   "metadata": {},
   "source": [
    "After reading a few of the reviews, I noticed that there's a lot of mixed language. Take this review as an example: \n",
    "\n",
    "2\tYT9tezwopYagEjTxIzN2dg\ti am just not a fan of this kind of pizza. i hate the sweet sauce, i hate that the ingredients are under the cheese so they don't get crunchy and crispy, the pepperoni is floppy. just not for me. the crust was kinda gooey like.   my delivery was 30 minutes late but its ok bc i wasn't in a hurry. they were really nice i just can't stand this kind of pizza.    if you like the sweet sauce and toppings under the cheese then go for it bc you'll probably love it!\n",
    "\n",
    "The review is part of the negative class, but it contains several positive words such as \"love\", \"nice\", and \"like\". This is true for a lot of the reviews I read. A lot of the reviews are balanced and not polarized. This makes classifying sentiment more challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bfd4c3-aad7-447f-85d9-9aa34fd93969",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130253ee-bd07-4194-ab50-0c6d7a5625ae",
   "metadata": {},
   "source": [
    "Below I train a Multinomial Naive Bayes model (\"bag of words model\") on the training set. To do this, I use sklearn, a popular open source machine learning package. I use sklearn's make_pipeline() method to create the model. The pipeline applies the functions passed in as arguments to the training data. In this case, it first applies CountVectorizer() to the training data to convert it into a matrix of counts. Finally, it applies MultinomialNB() to train the Multinomial NB model on the matrix of counts. The only parameters I provided are the reviews and ratings in the training data. I didn't modify the behavior of MultinomialNB(). By default, it sets alpha=1.0, which means it uses Laplace smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf1e062f-eaaf-4c8c-b20f-ccb53702096c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11468"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import metrics # Use this in part c to evaluate predictions.\n",
    "\n",
    "X_train = training_data[\"text\"]\n",
    "y_train = training_data[\"stars\"]\n",
    "multinomialnb_model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "# Train the model.\n",
    "multinomialnb_model.fit(X_train, y_train)\n",
    "\n",
    "multinomialnb_model[1].n_features_in_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69919fd-4ab4-4319-a662-6c5d94583bb4",
   "metadata": {},
   "source": [
    "The total number of features is shown above: 11,468. It's all the unique words in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e2ddd8-119a-4d00-8034-e78c1736ee62",
   "metadata": {},
   "source": [
    "Next, I print a feature representation for the first document in the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08e98868-190c-43d5-97fe-b791aac3c24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       feature  frequency\n",
      "0          the          6\n",
      "1          and          6\n",
      "2         menu          3\n",
      "3          not          3\n",
      "4        their          2\n",
      "5         good          2\n",
      "6           to          2\n",
      "7           it          2\n",
      "8         have          2\n",
      "9         were          2\n",
      "10       china          2\n",
      "11          on          2\n",
      "12    original          2\n",
      "13          is          2\n",
      "14      really          1\n",
      "15      recent          1\n",
      "16  restaurant          1\n",
      "17      return          1\n",
      "18         she          1\n",
      "19       price          1\n",
      "20     portion          1\n",
      "21        site          1\n",
      "22        size          1\n",
      "23          so          1\n",
      "24       again          1\n",
      "25       steak          1\n",
      "26       tasty          1\n",
      "27       phone          1\n",
      "28        this          1\n",
      "29        told          1\n",
      "30      trying          1\n",
      "31         tso          1\n",
      "32       under          1\n",
      "33      wanted          1\n",
      "34         web          1\n",
      "35        what          1\n",
      "36        when          1\n",
      "37       years          1\n",
      "38         yet          1\n",
      "39         you          1\n",
      "40       place          1\n",
      "41        once          1\n",
      "42      pepper          1\n",
      "43     general          1\n",
      "44     average          1\n",
      "45        both          1\n",
      "46    business          1\n",
      "47        chef          1\n",
      "48     chicken          1\n",
      "49    deserves          1\n",
      "50        does          1\n",
      "51     enjoyed          1\n",
      "52         fan          1\n",
      "53       first          1\n",
      "54        food          1\n",
      "55     forward          1\n",
      "56    friendly          1\n",
      "57       grand          1\n",
      "58       other          1\n",
      "59       great          1\n",
      "60          if          1\n",
      "61          in          1\n",
      "62       items          1\n",
      "63        just          1\n",
      "64        lady          1\n",
      "65        look          1\n",
      "66  management          1\n",
      "67     matched          1\n",
      "68         new          1\n",
      "69          of          1\n",
      "70      online          1\n",
      "71      opened          1\n",
      "72        your          1\n"
     ]
    }
   ],
   "source": [
    "# Load in dev data.\n",
    "dev_data = pd.read_csv('dev.tsv', sep = '\\t', header = None) \n",
    "\n",
    "# Add column names.\n",
    "dev_data.columns = [\"stars\", \"docid\", \"text\"]\n",
    "\n",
    "# Create CountVectorizer instance.\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the first document in the dev set.\n",
    "X = vectorizer.fit_transform([dev_data[\"text\"][0]])\n",
    "\n",
    "feature_rep = pd.DataFrame({\"feature\": vectorizer.get_feature_names_out(), \"frequency\": X.toarray()[0].tolist()})\n",
    "feature_rep = feature_rep.sort_values(by = ['frequency'], ascending = False)\n",
    "feature_rep = feature_rep.reset_index(drop=True)\n",
    "\n",
    "# Print all rows.\n",
    "print(feature_rep.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7846edf5-ef05-4892-ba72-683865aad225",
   "metadata": {},
   "source": [
    "Next, I print the docid and prediction (separated by a tab) for the first 10 documents in the dev file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aed1760-6528-4660-b3b1-8024f2fe033c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZSJnW6faaNFQoqq4ALqYg\t4\n",
      "Rcbv11hm5AYEwZyqYwAvg\t2\n",
      "rkRTjhu5szaBggeFVcVJlA\t4\n",
      "dhmeDsQGUS1FXMLs49SWjQ\t4\n",
      "z9zfIMYmRRCE4ggfOIieEw\t4\n",
      "Xtb3pGSh39bqcozkBECw\t2\n",
      "DOUflAGzxLsXG6xOmR1w\t2\n",
      "0RxCEWURe08CTcZt95F4AQ\t2\n",
      "MzUg5twEcCyd0X6lBMP2Lg\t2\n",
      "uNlw2D5CYKk0wjNxLtYw\t4\n"
     ]
    }
   ],
   "source": [
    "X_test = dev_data[\"text\"][0:10]\n",
    "\n",
    "predictions = multinomialnb_model.predict(X_test)\n",
    "\n",
    "for i in range(0, 10):\n",
    "    print(dev_data[\"docid\"][i] + \"\\t\" + str(predictions[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0b40d2-be82-4dfa-9115-aefa91c954a4",
   "metadata": {},
   "source": [
    "Finally, I make predictions for the dev and test partitions and write those to two files. The file names are dev_predictions_multinomialnb.tsv and test_predictions_multinomialnb.tsv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e558d72d-6751-4f0e-873d-ede4aace04ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for the full dev data.\n",
    "X_test = dev_data[\"text\"]\n",
    "predictions = multinomialnb_model.predict(X_test)\n",
    "\n",
    "# Write predictions to a file.\n",
    "predictions_df = pd.DataFrame({\"docid\": dev_data[\"docid\"], \"prediction\": predictions})\n",
    "predictions_df.to_csv(\"dev_predictions_multinomialnb.tsv\", sep = \"\\t\", index = False)\n",
    "\n",
    "# Make predictions for the test data.\n",
    "# Load in test data and add column names.\n",
    "test_data = pd.read_csv('test.tsv', sep = '\\t', header = None) \n",
    "test_data.columns = [\"stars\", \"docid\", \"text\"]\n",
    "X_test = test_data[\"text\"]\n",
    "predictions = multinomialnb_model.predict(X_test)\n",
    "\n",
    "# Write predictions to a file.\n",
    "predictions_df = pd.DataFrame({\"docid\": test_data[\"docid\"], \"prediction\": predictions})\n",
    "predictions_df.to_csv(\"test_predictions_multinomialnb.tsv\", sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087f30b0-1101-4b13-a6f1-cbdc99651fd5",
   "metadata": {},
   "source": [
    "## c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c166271-f5a9-4701-be4e-147c7091aa32",
   "metadata": {},
   "source": [
    "Next I evaluate the predictions of my Multinomial NB model from part b. I read in the predictions file (dev_predictions_multinomialnb.tsv) and calculate precision, recall, and F1 scores for the positive class (4 stars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86f2cb7f-815d-49d0-a6d9-ab0c94319cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statistic</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.835095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f1</td>\n",
       "      <td>0.811922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   statistic     value\n",
       "0  precision  0.835095\n",
       "1     recall  0.790000\n",
       "2         f1  0.811922"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the dev set predictions.\n",
    "predictions_dev = pd.read_csv('dev_predictions_multinomialnb.tsv', sep = '\\t')\n",
    "\n",
    "# Join prediction DF with dev data.\n",
    "combined_dev = pd.merge(dev_data, predictions_dev, on = \"docid\", how = \"inner\")\n",
    "\n",
    "# Rename \"stars\" column as \"actual\".\n",
    "combined_dev = combined_dev.rename(columns={\"stars\": \"actual\"})\n",
    "\n",
    "# Filter data so it only includes observations from the positive class.\n",
    "combined_dev_positive = combined_dev.loc[combined_dev[\"actual\"] == 4]\n",
    "combined_dev_negative = combined_dev.loc[combined_dev[\"actual\"] == 2]\n",
    "\n",
    "# Calculate precision. \n",
    "# precision = true positives/(true positives + false positives)\n",
    "n_true_positives = (combined_dev_positive[\"actual\"] == combined_dev_positive[\"prediction\"]).sum()\n",
    "n_false_positives = (combined_dev_negative[\"actual\"] != combined_dev_negative[\"prediction\"]).sum()\n",
    "precision = n_true_positives/(n_true_positives + n_false_positives)\n",
    "\n",
    "# Calculate recall.\n",
    "# recall = true positives/(true positives + false negatives)\n",
    "n_false_negatives = (combined_dev_positive[\"actual\"] != combined_dev_positive[\"prediction\"]).sum()\n",
    "recall = n_true_positives/(n_true_positives + n_false_negatives)\n",
    "\n",
    "# Calculate F1.\n",
    "f1 = (2 * precision * recall)/(precision + recall)\n",
    "\n",
    "# Report results.\n",
    "summary_stats_multinomialnb = pd.DataFrame({\"statistic\": [\"precision\", \"recall\", \"f1\"], \"value\": [precision, recall, f1]})\n",
    "summary_stats_multinomialnb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375b1d37-28b7-48a9-a3cc-b766c53d111a",
   "metadata": {},
   "source": [
    "Below I share some interesting mistakes my classifier made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3903801b-2ed9-4b24-bf00-5977df0e0c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>docid</th>\n",
       "      <th>text</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Rcbv11hm5AYEwZyqYwAvg</td>\n",
       "      <td>Meeting a friend for lunch, we had a little mi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>rkRTjhu5szaBggeFVcVJlA</td>\n",
       "      <td>Olive Garden used to be a favorite of the fami...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>0RxCEWURe08CTcZt95F4AQ</td>\n",
       "      <td>Our Las Vegas friend suggested duck tacos afte...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>uNlw2D5CYKk0wjNxLtYw</td>\n",
       "      <td>Stopped in here for lunch the other day. Quiet...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>HqtEtHHDgSM0ctJHehaWaw</td>\n",
       "      <td>I've never been a big fan of BK, I have held p...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>4</td>\n",
       "      <td>T1Lu6UeSHH6AZmzl9pnz7w</td>\n",
       "      <td>I had great service here today. I stopped in f...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>2</td>\n",
       "      <td>ggr9TDU0bVnZBEsiObxYEQ</td>\n",
       "      <td>Firstly, letme agree with Stephanie T. This pl...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>2</td>\n",
       "      <td>b1kLsgl6nc2blZXRAGjXw</td>\n",
       "      <td>Been there on several Friday nights and the fo...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>4</td>\n",
       "      <td>jV8aDYrQ4LFHb0b0hC3Iw</td>\n",
       "      <td>As soon as I walked in I was greeted by the sw...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>4</td>\n",
       "      <td>v6pqzYfd7XFM4t6fWI1wQA</td>\n",
       "      <td>This place is gonna cost me about $2,000.  ---...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>366 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual                   docid  \\\n",
       "1          4   Rcbv11hm5AYEwZyqYwAvg   \n",
       "2          2  rkRTjhu5szaBggeFVcVJlA   \n",
       "7          4  0RxCEWURe08CTcZt95F4AQ   \n",
       "9          2    uNlw2D5CYKk0wjNxLtYw   \n",
       "14         2  HqtEtHHDgSM0ctJHehaWaw   \n",
       "...      ...                     ...   \n",
       "1988       4  T1Lu6UeSHH6AZmzl9pnz7w   \n",
       "1992       2  ggr9TDU0bVnZBEsiObxYEQ   \n",
       "1993       2   b1kLsgl6nc2blZXRAGjXw   \n",
       "1998       4   jV8aDYrQ4LFHb0b0hC3Iw   \n",
       "1999       4  v6pqzYfd7XFM4t6fWI1wQA   \n",
       "\n",
       "                                                   text  prediction  \n",
       "1     Meeting a friend for lunch, we had a little mi...           2  \n",
       "2     Olive Garden used to be a favorite of the fami...           4  \n",
       "7     Our Las Vegas friend suggested duck tacos afte...           2  \n",
       "9     Stopped in here for lunch the other day. Quiet...           4  \n",
       "14    I've never been a big fan of BK, I have held p...           4  \n",
       "...                                                 ...         ...  \n",
       "1988  I had great service here today. I stopped in f...           2  \n",
       "1992  Firstly, letme agree with Stephanie T. This pl...           4  \n",
       "1993  Been there on several Friday nights and the fo...           4  \n",
       "1998  As soon as I walked in I was greeted by the sw...           2  \n",
       "1999  This place is gonna cost me about $2,000.  ---...           2  \n",
       "\n",
       "[366 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistakes = combined_dev.loc[combined_dev[\"actual\"] != combined_dev[\"prediction\"]]\n",
    "mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27ecc08-800a-4ac2-b46e-0f0815443d3b",
   "metadata": {},
   "source": [
    "My model misclassified 366 reviews, including following doc IDs: 0RxCEWURe08CTcZt95F4AQ, gjWVccNw6kB2UycZAEzyQg, rkRTjhu5szaBggeFVcVJlA, and ggr9TDU0bVnZBEsiObxYEQ. The first two reviews are actually positive, but my model classifies them as negative. The latter two reviews are actually negative, but my model classifies them as positive. Below are the reviews:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2019355-3450-4e19-b424-029b7a318d76",
   "metadata": {},
   "source": [
    "**Actual Class: Positive, Predicted Class: Negative**\n",
    "\n",
    "0RxCEWURe08CTcZt95F4AQ: Our Las Vegas friend suggested duck tacos after hanging out and having some drinks for a late night/ early morning (around 4am) snack.  All we needed to hear was tacos and we were in.  The small bar area of this place is open 24/7 so we stopped in for some half priced tapas (happy hour from midnight to 8am) and to chat with Dave, our friend's friend and an incredibly cool guy that was working the overnight shift.  It was 4am and I didn't want to be stuffed before going to bed, so I only ordered a few dishes.  I had the steak skewers with teriyaki glaze and the garlic cheese bread.  Both were good, but the garlic cheese bread was really good in its garlicy and cheesy-ness.  I also had one of the duck tacos from one of the others' plates and it was decent, but if I got it again I'd get it without the creamy tomato cilantro sauce.  Overall, I'd say the food is worth 3 or 3.5 stars, but Dave's coolness bumps this location up to a solid 4 stars.\n",
    "\n",
    "gjWVccNw6kB2UycZAEzyQg: I hate to find out that there was another location closer to where I was.  It doesn't matter because it was worth the trip.  I ordered pick up because I was by myself and wanted to a bit of exploring.  The place has a nice casual ambiance.  They also have an outdoor seating area.  I ordered the pad thai that comes with chicken and shrimp as a standard.  Since it was lunch time, it was part of the lunch menu that came with a choice of soup or salad.  I chose the soup, but was too stuffed to have it because I jumped right into the pad thai.  I ordered it medium spicy, and it was pretty spicy.  I can't imagine what hot would have tasted like.  Actually, I'm more curious of what it would have tasted like in a mild flavor.  The noodles were slightly overcooked, but still flavorful.  I also had the Thai Iced Tea here, and it was perfect for the weather and a great combination with the tangy pad thai flavor.  The prices are expensive compared to Chicago, but I hear it's typical for Phoenix/Scottsdale area.  My lunch speciai and drink costed me a little over $15.  I guess if you are craving it that bad like I was, you'll pay the price...\n",
    "\n",
    "**Actual Class: Negative, Predicted Class: Positive**\n",
    "\n",
    "rkRTjhu5szaBggeFVcVJlA: Olive Garden used to be a favorite of the family, recently they cut back the menu extensively and many of our favorites are gone.  I suggest checking the menu online before coming to see what's left.\n",
    "\n",
    "ggr9TDU0bVnZBEsiObxYEQ: Firstly, letme agree with Stephanie T. This place is far from 'rapid' at any time of the day. Whether there is a huge queue forming out of the door or just you in the whole shop, you will wait for what seems like an eternity before you are eventually served. Although to give credit where credit is due, the actual preperation time of the sandwiches is fairly quick - which may explain why they always look a bit messy and sloppy. However, once you stop being shallow about your food, the sandwiches here are actually pretty damn tasty and good value for what you pay for them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5906c4fb-a947-487b-b0a5-e0b8edd72329",
   "metadata": {},
   "source": [
    "The first two mistakes (actual class is positive) seem positive when considering the entire text, but there are a few negative words that may have led the model to misclassify them as negative. The first review (0RxCEWURe08CTcZt95F4AQ) includes the words \"late\" and \"but\", and the second review (gjWVccNw6kB2UycZAEzyQg) includes even stronger negative words like \"hate\" and \"overcooked\". Similarly, the second two mistakes (actual class is negative) include a few positive words that probably led the model to misclassify them. The first of these reviews (rkRTjhu5szaBggeFVcVJlA) uses the word \"favorite\", and the second review (ggr9TDU0bVnZBEsiObxYEQ) uses the words \"quick\", \"tasty\", and \"good\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6df54a-5787-44d0-b8ea-bfbae8eed705",
   "metadata": {},
   "source": [
    "## d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5552a1-f7f8-4f6b-8157-cb321be6e1e1",
   "metadata": {},
   "source": [
    "Next, I train an SVM model as an alternative to the Multinomial NB model above. I use the SVC class from sklearn to train the model. I first apply a tokenizer to the dev data, TfidfVectorizer(), to convert it into a matrix of TF-IDF scores. Next, I use the SVC class to train the SVM model on the matrix of TF-IDF scores. I use the default parameters during training, except for one modification. I set the kernel argument to \"linear\". This means the model uses a \"line\" (hyperplane in higher dimensions) as the decision boundary between classes. I chose a linear kernel because I saw it's widely recommended online for sentiment classification. Some of the reasons a linear kernel is recommended for sentiment classification are that: (1) text data have high dimensionality and can be separated with a line/hyperplane; (2) it avoids overfitting the training data; and (3) it's faster to train than other kernels (e.g. polynomial, sigmoid, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb95a4d2-4637-47e4-8299-477437620849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11468"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "X_train = training_data['text']\n",
    "y_train = training_data[\"stars\"]\n",
    "\n",
    "# Use the vectorizer below to find feature set for first document in dev set.\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "svm = make_pipeline(TfidfVectorizer(), SVC(kernel = \"linear\"))\n",
    "svm.fit(X_train, y_train)\n",
    "svm[1].n_features_in_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee170b7-3231-4f37-93c2-233097ea5a01",
   "metadata": {},
   "source": [
    "The total number of features is shown above: 11,468. It's all the unique words in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0a4d46-996d-4f39-a6d3-9ce117a33bf0",
   "metadata": {},
   "source": [
    "Next, I print a feature representation for the first document in the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f351d66-5d4a-4aad-86b9-343149eb9490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       feature    tf-idf\n",
      "0          the  0.436436\n",
      "1          and  0.436436\n",
      "2         menu  0.218218\n",
      "3          not  0.218218\n",
      "4        their  0.145479\n",
      "5         good  0.145479\n",
      "6           to  0.145479\n",
      "7           it  0.145479\n",
      "8         have  0.145479\n",
      "9         were  0.145479\n",
      "10       china  0.145479\n",
      "11          on  0.145479\n",
      "12    original  0.145479\n",
      "13          is  0.145479\n",
      "14      really  0.072739\n",
      "15      recent  0.072739\n",
      "16  restaurant  0.072739\n",
      "17      return  0.072739\n",
      "18         she  0.072739\n",
      "19       price  0.072739\n",
      "20     portion  0.072739\n",
      "21        site  0.072739\n",
      "22        size  0.072739\n",
      "23          so  0.072739\n",
      "24       again  0.072739\n",
      "25       steak  0.072739\n",
      "26       tasty  0.072739\n",
      "27       phone  0.072739\n",
      "28        this  0.072739\n",
      "29        told  0.072739\n",
      "30      trying  0.072739\n",
      "31         tso  0.072739\n",
      "32       under  0.072739\n",
      "33      wanted  0.072739\n",
      "34         web  0.072739\n",
      "35        what  0.072739\n",
      "36        when  0.072739\n",
      "37       years  0.072739\n",
      "38         yet  0.072739\n",
      "39         you  0.072739\n",
      "40       place  0.072739\n",
      "41        once  0.072739\n",
      "42      pepper  0.072739\n",
      "43     general  0.072739\n",
      "44     average  0.072739\n",
      "45        both  0.072739\n",
      "46    business  0.072739\n",
      "47        chef  0.072739\n",
      "48     chicken  0.072739\n",
      "49    deserves  0.072739\n",
      "50        does  0.072739\n",
      "51     enjoyed  0.072739\n",
      "52         fan  0.072739\n",
      "53       first  0.072739\n",
      "54        food  0.072739\n",
      "55     forward  0.072739\n",
      "56    friendly  0.072739\n",
      "57       grand  0.072739\n",
      "58       other  0.072739\n",
      "59       great  0.072739\n",
      "60          if  0.072739\n",
      "61          in  0.072739\n",
      "62       items  0.072739\n",
      "63        just  0.072739\n",
      "64        lady  0.072739\n",
      "65        look  0.072739\n",
      "66  management  0.072739\n",
      "67     matched  0.072739\n",
      "68         new  0.072739\n",
      "69          of  0.072739\n",
      "70      online  0.072739\n",
      "71      opened  0.072739\n",
      "72        your  0.072739\n"
     ]
    }
   ],
   "source": [
    "# Fit and transform the first document in the dev set.\n",
    "X = vectorizer.fit_transform([dev_data[\"text\"][0]])\n",
    "\n",
    "feature_rep = pd.DataFrame({\"feature\": vectorizer.get_feature_names_out(), \"tf-idf\": X.toarray()[0].tolist()})\n",
    "feature_rep = feature_rep.sort_values(by = ['tf-idf'], ascending = False)\n",
    "feature_rep = feature_rep.reset_index(drop=True)\n",
    "\n",
    "# Print all rows.\n",
    "print(feature_rep.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68572bf-791f-4995-8cbe-d35435037608",
   "metadata": {},
   "source": [
    "Next, I print the docid and prediction (separted by a tab) for the first 10 documents in the dev file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00bd1079-9a0e-43ae-bce2-8c1e26d74060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZSJnW6faaNFQoqq4ALqYg\t4\n",
      "Rcbv11hm5AYEwZyqYwAvg\t4\n",
      "rkRTjhu5szaBggeFVcVJlA\t4\n",
      "dhmeDsQGUS1FXMLs49SWjQ\t4\n",
      "z9zfIMYmRRCE4ggfOIieEw\t4\n",
      "Xtb3pGSh39bqcozkBECw\t2\n",
      "DOUflAGzxLsXG6xOmR1w\t2\n",
      "0RxCEWURe08CTcZt95F4AQ\t2\n",
      "MzUg5twEcCyd0X6lBMP2Lg\t2\n",
      "uNlw2D5CYKk0wjNxLtYw\t2\n"
     ]
    }
   ],
   "source": [
    "X_test = dev_data[\"text\"][0:10]\n",
    "\n",
    "predictions = svm.predict(X_test)\n",
    "\n",
    "for i in range(0, 10):\n",
    "    print(dev_data[\"docid\"][i] + \"\\t\" + str(predictions[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e30c04a-a97c-45a5-9358-4f0f010e6d23",
   "metadata": {},
   "source": [
    "Finally, I make predictions for the dev and test partitions and write those to two files. The file names are dev_predictions_svm.tsv and test_predictions_svm.tsv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebb4aae2-8e8a-48e6-8dcc-c8b81d1b3eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for the full dev data.\n",
    "X_test = dev_data[\"text\"]\n",
    "predictions = svm.predict(X_test)\n",
    "\n",
    "# Write predictions to a file.\n",
    "predictions_df = pd.DataFrame({\"docid\": dev_data[\"docid\"], \"prediction\": predictions})\n",
    "predictions_df.to_csv(\"dev_predictions_svm.tsv\", sep = \"\\t\", index = False)\n",
    "\n",
    "# Make predictions for the test data.\n",
    "X_test = test_data[\"text\"]\n",
    "predictions = svm.predict(X_test)\n",
    "\n",
    "# Write predictions to a file.\n",
    "predictions_df = pd.DataFrame({\"docid\": test_data[\"docid\"], \"prediction\": predictions})\n",
    "predictions_df.to_csv(\"test_predictions_svm.tsv\", sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e835122-2b2a-412d-b674-d69ecdfbd2c9",
   "metadata": {},
   "source": [
    "Next, I evaluate the predictions, calculating precision, recall, and F1 for the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54dc94bb-01c6-4524-b404-d6792c33aacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statistic</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.847336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.827000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f1</td>\n",
       "      <td>0.837045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   statistic     value\n",
       "0  precision  0.847336\n",
       "1     recall  0.827000\n",
       "2         f1  0.837045"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the dev set predictions.\n",
    "predictions_dev = pd.read_csv('dev_predictions_svm.tsv', sep = '\\t')\n",
    "\n",
    "# Join prediction DF with dev data.\n",
    "combined_dev = pd.merge(dev_data, predictions_dev, on = \"docid\", how = \"inner\")\n",
    "\n",
    "# Rename \"stars\" column as \"actual\".\n",
    "combined_dev = combined_dev.rename(columns={\"stars\": \"actual\"})\n",
    "\n",
    "# Filter data so it only includes observations from the positive class.\n",
    "combined_dev_positive = combined_dev.loc[combined_dev[\"actual\"] == 4]\n",
    "combined_dev_negative = combined_dev.loc[combined_dev[\"actual\"] == 2]\n",
    "\n",
    "# Calculate precision. \n",
    "# precision = true positives/(true positives + false positives)\n",
    "n_true_positives = (combined_dev_positive[\"actual\"] == combined_dev_positive[\"prediction\"]).sum()\n",
    "n_false_positives = (combined_dev_negative[\"actual\"] != combined_dev_negative[\"prediction\"]).sum()\n",
    "precision = n_true_positives/(n_true_positives + n_false_positives)\n",
    "\n",
    "# Calculate recall.\n",
    "# recall = true positives/(true positives + false negatives)\n",
    "n_false_negatives = (combined_dev_positive[\"actual\"] != combined_dev_positive[\"prediction\"]).sum()\n",
    "recall = n_true_positives/(n_true_positives + n_false_negatives)\n",
    "\n",
    "# Calculate F1.\n",
    "f1 = (2 * precision * recall)/(precision + recall)\n",
    "\n",
    "# Report results.\n",
    "summary_stats_svm = pd.DataFrame({\"statistic\": [\"precision\", \"recall\", \"f1\"], \"value\": [precision, recall, f1]})\n",
    "summary_stats_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbcf1a0-3f34-47fd-9d20-bbf4a6954544",
   "metadata": {},
   "source": [
    "Next, I show some interesting mistakes the SVM model made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bacfcd0-e016-4690-9a32-ec86ab64c028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>docid</th>\n",
       "      <th>text</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>rkRTjhu5szaBggeFVcVJlA</td>\n",
       "      <td>Olive Garden used to be a favorite of the fami...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>0RxCEWURe08CTcZt95F4AQ</td>\n",
       "      <td>Our Las Vegas friend suggested duck tacos afte...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>HqtEtHHDgSM0ctJHehaWaw</td>\n",
       "      <td>I've never been a big fan of BK, I have held p...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>gjWVccNw6kB2UycZAEzyQg</td>\n",
       "      <td>I hate to find out that there was another loca...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>JemvAfRVgCASkI3uG3Vffg</td>\n",
       "      <td>Been here several times. Hasn't let me down ye...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>4</td>\n",
       "      <td>T1Lu6UeSHH6AZmzl9pnz7w</td>\n",
       "      <td>I had great service here today. I stopped in f...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>2</td>\n",
       "      <td>ggr9TDU0bVnZBEsiObxYEQ</td>\n",
       "      <td>Firstly, letme agree with Stephanie T. This pl...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>2</td>\n",
       "      <td>b1kLsgl6nc2blZXRAGjXw</td>\n",
       "      <td>Been there on several Friday nights and the fo...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>4</td>\n",
       "      <td>jV8aDYrQ4LFHb0b0hC3Iw</td>\n",
       "      <td>As soon as I walked in I was greeted by the sw...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>4</td>\n",
       "      <td>v6pqzYfd7XFM4t6fWI1wQA</td>\n",
       "      <td>This place is gonna cost me about $2,000.  ---...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>322 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual                   docid  \\\n",
       "2          2  rkRTjhu5szaBggeFVcVJlA   \n",
       "7          4  0RxCEWURe08CTcZt95F4AQ   \n",
       "14         2  HqtEtHHDgSM0ctJHehaWaw   \n",
       "22         4  gjWVccNw6kB2UycZAEzyQg   \n",
       "23         4  JemvAfRVgCASkI3uG3Vffg   \n",
       "...      ...                     ...   \n",
       "1988       4  T1Lu6UeSHH6AZmzl9pnz7w   \n",
       "1992       2  ggr9TDU0bVnZBEsiObxYEQ   \n",
       "1993       2   b1kLsgl6nc2blZXRAGjXw   \n",
       "1998       4   jV8aDYrQ4LFHb0b0hC3Iw   \n",
       "1999       4  v6pqzYfd7XFM4t6fWI1wQA   \n",
       "\n",
       "                                                   text  prediction  \n",
       "2     Olive Garden used to be a favorite of the fami...           4  \n",
       "7     Our Las Vegas friend suggested duck tacos afte...           2  \n",
       "14    I've never been a big fan of BK, I have held p...           4  \n",
       "22    I hate to find out that there was another loca...           2  \n",
       "23    Been here several times. Hasn't let me down ye...           2  \n",
       "...                                                 ...         ...  \n",
       "1988  I had great service here today. I stopped in f...           2  \n",
       "1992  Firstly, letme agree with Stephanie T. This pl...           4  \n",
       "1993  Been there on several Friday nights and the fo...           4  \n",
       "1998  As soon as I walked in I was greeted by the sw...           2  \n",
       "1999  This place is gonna cost me about $2,000.  ---...           2  \n",
       "\n",
       "[322 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistakes = combined_dev.loc[combined_dev[\"actual\"] != combined_dev[\"prediction\"]]\n",
    "mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349e6ec8-3a5d-40aa-83b0-9abb76ffd082",
   "metadata": {},
   "source": [
    "**Actual Class: Positive, Predicted Class: Negative**\n",
    "\n",
    "JemvAfRVgCASkI3uG3Vffg: Been here several times. Hasn't let me down yet. It's easily accessible off Boulder Highway and is open 24/7. This is one of my preferred spots to redeem the BOGO coupon for the Double Del Cheeseburger, $3.00 each, and I don't recall having any issues. They also aren't upselling jerks who pester you about trying a new item or asking 3 times whether you're sure that you've concluded your order.\n",
    "\n",
    "v6pqzYfd7XFM4t6fWI1wQA: This place is gonna cost me about $2,000.  ---- Back story, I was backpacking around South America a few years ago and spent a few weeks in Ecuador. Loved it. Great place.  Of course I ate Ecuadorian food but don't remember much about it.   Now after eating at MCMP I figure it's gonna be about $2,000 for me to go back to Ecuador and try the real stuff again so I can compare the two.  Anyways, loved the oatmeal drink. It tasted like pineapple juice with a hint of spice or cinnamon. Chips and salsa were standard issue.  The two dinners we had were very good. One was like a spicy carne asada, the other was sort of a combo plate with beef, beans, two types of plantains, sausage, egg, rice and chichron.  Our entrees were fine, but I'm really excited to try the other items on the menu.  Empanadas, tamales, and the chiviche looked really good.  This place has about 6-8 tables that's it. Only one person to serve everyone so if the place is packed expect a few more minutes for everything.\n",
    "\n",
    "\n",
    "**Actual Class: Negative, Predicted Class: Positive**\n",
    "\n",
    "rkRTjhu5szaBggeFVcVJlA: Olive Garden used to be a favorite of the family, recently they cut back the menu extensively and many of our favorites are gone.  I suggest checking the menu online before coming to see what's left.\n",
    "\n",
    "ggr9TDU0bVnZBEsiObxYEQ: Firstly, letme agree with Stephanie T. This place is far from 'rapid' at any time of the day. Whether there is a huge queue forming out of the door or just you in the whole shop, you will wait for what seems like an eternity before you are eventually served. Although to give credit where credit is due, the actual preperation time of the sandwiches is fairly quick - which may explain why they always look a bit messy and sloppy. However, once you stop being shallow about your food, the sandwiches here are actually pretty damn tasty and good value for what you pay for them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8775a0-8ee3-4542-8457-a4837f1c318e",
   "metadata": {},
   "source": [
    "Like the earlier mistakes I showed for the first model (Multinomial NB), some of these examples have mixed language. The first review has the words \"issues\" and \"upselling\". It's not clear why the second example was mistakenly classified as negative. The word \"but\" appears in the review. Otherwise, it seems positive. The third and fourth reviews have positive words like \"favorite\", \"quick\", and \"tasty\", so it's easier to see why the SVM model misclassified them as positive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306900a7-a207-489c-a25a-c49b4470ca53",
   "metadata": {},
   "source": [
    "I show the performance of both models below (summary stats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3bd3849-687b-48f9-baf6-48fcea5015ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "   statistic     value\n",
      "0  precision  0.835095\n",
      "1     recall  0.790000\n",
      "2         f1  0.811922\n",
      "\n",
      "SVM\n",
      "   statistic     value\n",
      "0  precision  0.847336\n",
      "1     recall  0.827000\n",
      "2         f1  0.837045\n"
     ]
    }
   ],
   "source": [
    "print(\"Multinomial NB\")\n",
    "print(summary_stats_multinomialnb)\n",
    "print(\"\")\n",
    "print(\"SVM\")\n",
    "print(summary_stats_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e854dd5-5c9c-49f2-94dd-f9d4ea962037",
   "metadata": {},
   "source": [
    "The SVM model performs better across all performance metrics than the Multinomial NB model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a3450b-2848-4436-a9d3-f5ab8a756b90",
   "metadata": {},
   "source": [
    "## e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8357a1ba-3036-4caa-b0a4-daa508e05bd5",
   "metadata": {},
   "source": [
    "I improve on the SVM model from part d by including word bigrams in the set of features. I do this by modifying the ngram_range argument in the TfidfVectorizer constructor. By default, the tokenizer only uses word unigrams. I set it to the tuple (1, 2), which means the minimum ngram the tokenizer extracts from the training data is a unigram, and the maximum ngram it extracts from the training data is a bigram. I train the model, make predictions on the dev set, and evaluate the predictions on the dev set (positive class only) alongside the two previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8df59516-ad36-45f7-ba33-43e54ae9a394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112436"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = training_data['text']\n",
    "y_train = training_data[\"stars\"]\n",
    "\n",
    "svm_bigram = make_pipeline(TfidfVectorizer(ngram_range = (1, 2)), SVC(kernel = \"linear\"))\n",
    "svm_bigram.fit(X_train, y_train)\n",
    "svm_bigram[1].n_features_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31b9074f-f473-4ae0-8b64-9f6f68ebfecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for the full dev data.\n",
    "X_test = dev_data[\"text\"]\n",
    "predictions = svm_bigram.predict(X_test)\n",
    "\n",
    "# Write predictions to a file.\n",
    "predictions_df = pd.DataFrame({\"docid\": dev_data[\"docid\"], \"prediction\": predictions})\n",
    "predictions_df.to_csv(\"dev_predictions_svm_bigram.tsv\", sep = \"\\t\", index = False)\n",
    "\n",
    "# Make predictions for the test data.\n",
    "X_test = test_data[\"text\"]\n",
    "predictions = svm_bigram.predict(X_test)\n",
    "\n",
    "# Write predictions to a file.\n",
    "predictions_df = pd.DataFrame({\"docid\": test_data[\"docid\"], \"prediction\": predictions})\n",
    "predictions_df.to_csv(\"test_predictions_svm_bigram.tsv\", sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f71bb1fc-2a7b-4f38-b80f-19c82821721f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "   statistic     value\n",
      "0  precision  0.835095\n",
      "1     recall  0.790000\n",
      "2         f1  0.811922\n",
      "\n",
      "SVM\n",
      "   statistic     value\n",
      "0  precision  0.847336\n",
      "1     recall  0.827000\n",
      "2         f1  0.837045\n",
      "\n",
      "SVM Bigram\n",
      "   statistic     value\n",
      "0  precision  0.870540\n",
      "1     recall  0.854000\n",
      "2         f1  0.862191\n"
     ]
    }
   ],
   "source": [
    "# Read in the dev set predictions.\n",
    "predictions_dev = pd.read_csv('dev_predictions_svm_bigram.tsv', sep = '\\t')\n",
    "\n",
    "# Join prediction DF with dev data.\n",
    "combined_dev = pd.merge(dev_data, predictions_dev, on = \"docid\", how = \"inner\")\n",
    "\n",
    "# Rename \"stars\" column as \"actual\".\n",
    "combined_dev = combined_dev.rename(columns={\"stars\": \"actual\"})\n",
    "\n",
    "# Filter data so it only includes observations from the positive class.\n",
    "combined_dev_positive = combined_dev.loc[combined_dev[\"actual\"] == 4]\n",
    "combined_dev_negative = combined_dev.loc[combined_dev[\"actual\"] == 2]\n",
    "\n",
    "# Calculate precision. \n",
    "# precision = true positives/(true positives + false positives)\n",
    "n_true_positives = (combined_dev_positive[\"actual\"] == combined_dev_positive[\"prediction\"]).sum()\n",
    "n_false_positives = (combined_dev_negative[\"actual\"] != combined_dev_negative[\"prediction\"]).sum()\n",
    "precision = n_true_positives/(n_true_positives + n_false_positives)\n",
    "\n",
    "# Calculate recall.\n",
    "# recall = true positives/(true positives + false negatives)\n",
    "n_false_negatives = (combined_dev_positive[\"actual\"] != combined_dev_positive[\"prediction\"]).sum()\n",
    "recall = n_true_positives/(n_true_positives + n_false_negatives)\n",
    "\n",
    "# Calculate F1.\n",
    "f1 = (2 * precision * recall)/(precision + recall)\n",
    "\n",
    "# Report results.\n",
    "summary_stats_svm_bigram = pd.DataFrame({\"statistic\": [\"precision\", \"recall\", \"f1\"], \"value\": [precision, recall, f1]})\n",
    "print(\"Multinomial NB\")\n",
    "print(summary_stats_multinomialnb)\n",
    "print(\"\")\n",
    "print(\"SVM\")\n",
    "print(summary_stats_svm)\n",
    "print(\"\")\n",
    "print(\"SVM Bigram\")\n",
    "print(summary_stats_svm_bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18490c05-9be2-4711-a52d-f9e7f185054a",
   "metadata": {},
   "source": [
    "The modified SVM model that uses bigrams performs better than both of the previous models on all the evaluation metrics. It has higher precision, recall, and F1 scores when considering observations from the positive class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37df1189-f9aa-4d25-8daf-70a40c401ee1",
   "metadata": {},
   "source": [
    "Finally, I use the modified SVM model to make predictions on the test set, and I write the predictions to zproom1.tsv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cda419dc-72c5-4311-af4c-1c1cc4f1b390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for the full test data.\n",
    "X_test = test_data[\"text\"]\n",
    "predictions = svm_bigram.predict(X_test)\n",
    "\n",
    "# Write predictions to a file.\n",
    "predictions_df = pd.DataFrame({\"docid\": dev_data[\"docid\"], \"prediction\": predictions})\n",
    "predictions_df.to_csv(\"zproom1.tsv\", sep = \"\\t\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
