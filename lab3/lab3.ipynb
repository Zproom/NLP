{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "628739d4-eda3-4434-9b6a-ffae99535986",
   "metadata": {},
   "source": [
    "Zachary Proom\n",
    "\n",
    "EN.605.646.81: Natural Language Processing\n",
    "\n",
    "# Lab #3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405a2c93-5845-4cd9-a426-c21ea608166a",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e095e2-fdb3-40ae-bbff-fb0b1336c0db",
   "metadata": {},
   "source": [
    "First, I split the training data in train.tsv into two groups, based on the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "addb988d-6528-4b94-8413-a98618c24c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fc6e229b-3fc7-4b71-863f-d11cff845dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in train.tsv.\n",
    "training_data = pd.read_csv('train.tsv', sep = '\\t', header = None) \n",
    "\n",
    "# Add column names.\n",
    "training_data.columns = [\"stars\", \"docid\", \"text\"]\n",
    "\n",
    "# Split into two groups based on class.\n",
    "training_data_negative = training_data.loc[training_data[\"stars\"] == 2]\n",
    "training_data_positive = training_data.loc[training_data[\"stars\"] == 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800c3fd2-566a-4440-9d72-c8f6a674355d",
   "metadata": {},
   "source": [
    "I found ten words that indicate positive or negative sentiment, and I show their relative frequencies in the table below (i.e. the percent of reviews they appear in in each class). The first five words indicate positive sentiment, and the last five indicate negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1c0e4cae-c0b8-4796-9f01-ce44695efa9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amazing</td>\n",
       "      <td>5.2</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>awesome</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>great</td>\n",
       "      <td>35.9</td>\n",
       "      <td>17.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>incredible</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fantastic</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>terrible</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>horrible</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>worst</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>awful</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bad</td>\n",
       "      <td>7.1</td>\n",
       "      <td>16.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word  positive  negative\n",
       "0     amazing       5.2       1.8\n",
       "1     awesome       5.7       2.2\n",
       "2       great      35.9      17.3\n",
       "3  incredible       0.4       0.1\n",
       "4   fantastic       3.7       1.0\n",
       "5    terrible       0.3       3.2\n",
       "6    horrible       0.6       3.1\n",
       "7       worst       0.7       4.7\n",
       "8       awful       0.6       2.5\n",
       "9         bad       7.1      16.7"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_freqs = pd.DataFrame(columns = ['word', 'positive', 'negative'])\n",
    "\n",
    "for word in [\"amazing\", \"awesome\", \"great\", \"incredible\", \"fantastic\", \"terrible\", \"horrible\", \"worst\", \"awful\", \"bad\"]:\n",
    "    negative_freq = sum(training_data_negative['text'].str.contains(word))/len(training_data_negative) * 100\n",
    "    positive_freq = sum(training_data_positive['text'].str.contains(word))/len(training_data_positive) * 100\n",
    "    relative_freqs = pd.concat([relative_freqs, pd.DataFrame({\"word\": [word], \"positive\": [positive_freq], \"negative\": [negative_freq]})], ignore_index = True)\n",
    "\n",
    "relative_freqs = relative_freqs.reset_index(drop=True)\n",
    "relative_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2fc7c7-6e19-49b2-aa50-f8c518803823",
   "metadata": {},
   "source": [
    "After reading a few of the reviews, I noticed that there's a lot of mixed language. Take this review as an example: \n",
    "\n",
    "2\tYT9tezwopYagEjTxIzN2dg\ti am just not a fan of this kind of pizza. i hate the sweet sauce, i hate that the ingredients are under the cheese so they don't get crunchy and crispy, the pepperoni is floppy. just not for me. the crust was kinda gooey like.   my delivery was 30 minutes late but its ok bc i wasn't in a hurry. they were really nice i just can't stand this kind of pizza.    if you like the sweet sauce and toppings under the cheese then go for it bc you'll probably love it!\n",
    "\n",
    "The review is part of the negative class, but it contains several positive words such as \"love\", \"nice\", and \"like\". This is true for a lot of the reviews I read. A lot of the reviews are balanced and not polarized. This makes classifying sentiment more challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bfd4c3-aad7-447f-85d9-9aa34fd93969",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130253ee-bd07-4194-ab50-0c6d7a5625ae",
   "metadata": {},
   "source": [
    "Below I train a Multinomial Naive Bayes model (\"bag of words model\") on the training set. To do this, I use sklearn, a popular open source machine learning package. I use sklearn's make_pipeline() function to create the model. The pipeline applies the functions passed in as arguments to the training data. In this case, it first applies CountVectorizer() to the training data to convert it into a matrix of counts. Finally, it applies MultinomialNB() to train the Multinomial NB model on the matrix of counts. The only parameters I provided are the reviews and ratings in the training data. I didn't modify the behavior of MultinomialNB(). By default, it sets alpha=1.0, which means it uses Laplace smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cf1e062f-eaaf-4c8c-b20f-ccb53702096c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11468"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "labels = [4, 2] # 4 for positive, 2 for negative\n",
    "X_train = training_data[\"text\"]\n",
    "y_train = training_data[\"stars\"]\n",
    "multinomialnb_model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "# Train the model.\n",
    "multinomialnb_model.fit(X_train, y_train)\n",
    "\n",
    "multinomialnb_model[1].n_features_in_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69919fd-4ab4-4319-a662-6c5d94583bb4",
   "metadata": {},
   "source": [
    "The total number of features is shown above: 11,468. It's all the unique words in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e2ddd8-119a-4d00-8034-e78c1736ee62",
   "metadata": {},
   "source": [
    "Next, I print a feature representation for the first document in the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "08e98868-190c-43d5-97fe-b791aac3c24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       feature  frequency\n",
      "0          the          6\n",
      "1          and          6\n",
      "2         menu          3\n",
      "3          not          3\n",
      "4        their          2\n",
      "5         good          2\n",
      "6           to          2\n",
      "7           it          2\n",
      "8         have          2\n",
      "9         were          2\n",
      "10       china          2\n",
      "11          on          2\n",
      "12    original          2\n",
      "13          is          2\n",
      "14      really          1\n",
      "15      recent          1\n",
      "16  restaurant          1\n",
      "17      return          1\n",
      "18         she          1\n",
      "19       price          1\n",
      "20     portion          1\n",
      "21        site          1\n",
      "22        size          1\n",
      "23          so          1\n",
      "24       again          1\n",
      "25       steak          1\n",
      "26       tasty          1\n",
      "27       phone          1\n",
      "28        this          1\n",
      "29        told          1\n",
      "30      trying          1\n",
      "31         tso          1\n",
      "32       under          1\n",
      "33      wanted          1\n",
      "34         web          1\n",
      "35        what          1\n",
      "36        when          1\n",
      "37       years          1\n",
      "38         yet          1\n",
      "39         you          1\n",
      "40       place          1\n",
      "41        once          1\n",
      "42      pepper          1\n",
      "43     general          1\n",
      "44     average          1\n",
      "45        both          1\n",
      "46    business          1\n",
      "47        chef          1\n",
      "48     chicken          1\n",
      "49    deserves          1\n",
      "50        does          1\n",
      "51     enjoyed          1\n",
      "52         fan          1\n",
      "53       first          1\n",
      "54        food          1\n",
      "55     forward          1\n",
      "56    friendly          1\n",
      "57       grand          1\n",
      "58       other          1\n",
      "59       great          1\n",
      "60          if          1\n",
      "61          in          1\n",
      "62       items          1\n",
      "63        just          1\n",
      "64        lady          1\n",
      "65        look          1\n",
      "66  management          1\n",
      "67     matched          1\n",
      "68         new          1\n",
      "69          of          1\n",
      "70      online          1\n",
      "71      opened          1\n",
      "72        your          1\n"
     ]
    }
   ],
   "source": [
    "# Load in dev data.\n",
    "dev_data = pd.read_csv('dev.tsv', sep = '\\t', header = None) \n",
    "\n",
    "# Add column names.\n",
    "dev_data.columns = [\"stars\", \"docid\", \"text\"]\n",
    "\n",
    "# Create CountVectorizer instance.\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the first document in the dev set.\n",
    "X = vectorizer.fit_transform([dev_data[\"text\"][0]])\n",
    "\n",
    "feature_rep = pd.DataFrame({\"feature\": vectorizer.get_feature_names_out(), \"frequency\": X.toarray()[0].tolist()})\n",
    "feature_rep = feature_rep.sort_values(by = ['frequency'], ascending = False)\n",
    "feature_rep = feature_rep.reset_index(drop=True)\n",
    "\n",
    "# Print all rows.\n",
    "print(feature_rep.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7846edf5-ef05-4892-ba72-683865aad225",
   "metadata": {},
   "source": [
    "Next, I print the docid and prediction (separted by a tab) for the first 10 documents in the dev file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8aed1760-6528-4660-b3b1-8024f2fe033c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZSJnW6faaNFQoqq4ALqYg\t4\n",
      "Rcbv11hm5AYEwZyqYwAvg\t2\n",
      "rkRTjhu5szaBggeFVcVJlA\t4\n",
      "dhmeDsQGUS1FXMLs49SWjQ\t4\n",
      "z9zfIMYmRRCE4ggfOIieEw\t4\n",
      "Xtb3pGSh39bqcozkBECw\t2\n",
      "DOUflAGzxLsXG6xOmR1w\t2\n",
      "0RxCEWURe08CTcZt95F4AQ\t2\n",
      "MzUg5twEcCyd0X6lBMP2Lg\t2\n",
      "uNlw2D5CYKk0wjNxLtYw\t4\n"
     ]
    }
   ],
   "source": [
    "X_test = dev_data[\"text\"][0:10]\n",
    "\n",
    "predictions = multinomialnb_model.predict(X_test)\n",
    "\n",
    "for i in range(0, 10):\n",
    "    print(dev_data[\"docid\"][i] + \"\\t\" + str(predictions[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0b40d2-be82-4dfa-9115-aefa91c954a4",
   "metadata": {},
   "source": [
    "Finally, I make predictions for the dev and test partitions and write those to two files. The file names are dev_predictions_multinomialnb.tsv and test_predictions_multinomialnb.tsv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e558d72d-6751-4f0e-873d-ede4aace04ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for the full dev data.\n",
    "X_test = dev_data[\"text\"]\n",
    "predictions = multinomialnb_model.predict(X_test)\n",
    "\n",
    "# Write predictions to a file.\n",
    "predictions_df = pd.DataFrame({\"docid\": dev_data[\"docid\"], \"prediction\": predictions})\n",
    "predictions_df.to_csv(\"dev_predictions_multinomialnb.tsv\", sep = \"\\t\")\n",
    "\n",
    "# Make predictions for the test data.\n",
    "# Load in test data and add column names.\n",
    "test_data = pd.read_csv('test.tsv', sep = '\\t', header = None) \n",
    "test_data.columns = [\"stars\", \"docid\", \"text\"]\n",
    "X_test = test_data[\"text\"]\n",
    "predictions = multinomialnb_model.predict(X_test)\n",
    "\n",
    "# Write predictions to a file.\n",
    "predictions_df = pd.DataFrame({\"docid\": test_data[\"docid\"], \"prediction\": predictions})\n",
    "predictions_df.to_csv(\"test_predictions_multinomialnb.tsv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087f30b0-1101-4b13-a6f1-cbdc99651fd5",
   "metadata": {},
   "source": [
    "## c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
